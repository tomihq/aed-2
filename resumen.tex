\documentclass[10pt,a4paper]{article}
\usepackage{blindtext}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{circuitikz}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\input{AEDmacros}
\newcommand{\notimplies}{\;\not\!\!\!\implies}
\title{Algoritmos y Estructuras de Datos II}
\author{Tomás Agustín Hernández}
\date{}

\begin{document}
\maketitle

\begin{figure}[b]
    \centering
    \begin{tikzpicture}[remember picture,overlay]
        \node[anchor=south east, inner sep=0pt, xshift=-1cm, yshift=2cm] at (current page.south east) {
            \begin{minipage}[b]{0.5\textwidth}
                \includegraphics[width=\linewidth]{logo_uba.jpg}
                \label{fig:bottom}
            \end{minipage}
        };
    \end{tikzpicture}
\end{figure}

\newpage
\section{Especificación}
\subsection*{Consideraciones importantes / Reminders}
\begin{itemize}
    \item Utilizar operadores luego: Si estoy en LPO (Lógica de Primer Orden) utilizar los operadores luego si vemos que hay una posible indefinición como una división, o ingresar a una lista a un índice. Recordar que el para todo y un existe, aunque esté acotado por un rango, los cuantificadores predican IGUAL para todos los valores. Entonces, aunque diga que x es positivo, también probará dividir inclusive por 0 y estallará.
    \item Recordar las condiciones bidireccionales
        \begin{itemize}
            \item Si por algún motivo tengo que armar una “lista”, como, por ejemplo, los divisores de un número x tengo que indicar que, si el número divide a x, entonces ese número está en res, pero además todos los valores que están en res DIVIDEN a x. Es una condición bidireccional. 
            \item Otro ejemplo puede ser que tenga que considerar el máximo de una lista, si todos los valores y que están en la lista son menores que res entonces significa que res también pertenece a esa lista original.
        \end{itemize}
    \item Recordar el significado de los cuantificadores con dos variables al mismo tiempo: En la lógica se ejecutan todos de uno a la vez. Es decir, si tengo que poner un para todo adentro de un para todo entonces hago un para todo solo con dos variables y listo.
    \item Recordar que cuando en un procedimiento llamo a un predicado y ese predicado devuelve algo de un para todo, existe (básicamente un valor de verdad) tengo que castear ese valor en el procedimiento porque son dos mundos distintos.
    Ej: asegura: { res = True \(\iff\) predicado}
    \item Los predicados y funciones auxiliares no describen problemas. Son herramientas sintácticas para descomponer predicados.
    \begin{itemize}
        \item Los procedimientos pueden llamar a funciones auxiliares o predicados. Un procedimiento no puede llamar a otro procedimiento.
        \item Los predicados pueden llamar a predicados o auxiliares. 
        \item Las auxiliares solo pueden llamar auxiliares.
    \end{itemize}
    \item No usamos nunca \(==\) en especificación, usamos siempre \(=\) y estamos comparando, no asignando.
    \item No existe el guardar o asignar en el mundo de la lógica. No puedo guardar en una lista en un índice específico porque si un valor. Para esto solemos usar que x valor pertenecerá a esta lista, por ejemplo.
    \item Si tengo un algoritmo que cumple una funcionalidad específica con un require más débil, puedo poner el require más restrictivo y va a funcionar igual pero NO al revés.
\end{itemize}
\subsection*{Fórmulas compuestas}
Decimos que una fórmula es compuesta a una fórmula que tiene más de una operación y esa operación necesita realizarse antes de conocer su valor.
\begin{itemize}
    \item \((p \land q) \lor m\)
    \item \(((p \land q) \lor m) \implies n\)
\end{itemize}
\subsection*{Fórmula atómica}
Decimos que una fórmula es atómica si se puede inferir su valor con una, o ninguna operación. Es irreducible.
\begin{itemize}
    \item p
    \item \(p \land q\)
\end{itemize}
\subsection*{Fórmulas bien definidas}
Decimos que una fórmula está bien definida cuando el orden que hay que hacer las operaciones es clara. Es decir, cuando cada operación toma dos variables proposicionales, y al realizar la operación termina siendo una fórmula atómica.
\begin{itemize}
    \item \(p \land q \lor r\) está mal formada. No se especifica si primero se realiza el \(\land\) o el \(\lor\).
    \item \((p \land q) \lor r\) está bien formada.
    \item \(p \land q \land r \land m\) está bien formada porque son todas conjunciones.
    \item \(p \lor q \lor r \lor m\) está bien formada porque son todas disyunciones.
\end{itemize}
\subsection*{Cuantificadores}
\begin{itemize}
    \item Para todo: \(\forall\)
    \begin{itemize}
        \item \(Garantiza \ la \ \text{conjunción}: p(1) \land p(2) \land p(3) \dots \land p(m) \). Todos los casos deben ser \True \ para que el cuantificador sea \True.
        \item Se acompaña por un \(\implica\) a la hora de predicar sobre los elementos.
        \item \((\forall i: \ent)(0 \le i < \longitud{s} \implicaLuego \ s[i] \ mod \ 2 = 0)\). Todos los elementos de la lista son divisibles por 2.
        \item Estructura: \(\forall\) + rango + \(\implicaLuego\)
    \end{itemize}
    \item Existe: \(\exists\)
    \begin{itemize}
        \item \(Garantiza \ la \ \text{disyunción}: p(1) \lor p(2) \lor p(3) \dots \lor p(m) \). Con un caso \True \ el cuantificador es \True.
        \item Se acompaña por un \(\land\) a la hora de predicar sobre los elementos.
        \item \((\exists i: \ent)(0 \le i < \longitud{s} \yLuego s[i] \ge 0)\). Existe algún elemento en la lista que es mayor o igual a 0.
        \item \(\exists\) + rango + \(\yLuego\)
    \end{itemize}
\end{itemize}
\subsection*{Equivalencias entre fórmulas}
Decimos que dos fórmulas son equivalentes \(\iff\) los valores de la tabla de verdad al aplicar la operación arroja el mismo resultado.

\subsection*{Valuaciones}
Las valuaciones surgen en base a la tabla de verdad. Las valuaciones serian darle valor a las variables proposicionales y ver el resultado de la operación. Solo hacen referencias a fórmulas atómicas.

\subsection*{Tautologias, contradicciones y contingencias}
\begin{itemize}
    \item Una fórmula es tautología \(\iff\) el resultado de la operación en cada fila arroja siempre V.
    \item Una fórmula es contradicción \(\iff\) el resultado de la operación en cada fila arroja siempre F.
    \item  Una fórmula es contradicción \(\iff\) el resultado de la operación en cada fila arroja siempre V y F.
\end{itemize}

\subsection*{Relaciones de fuerza entre fórmulas}
Decimos que una fórmula es más fuerte que la otra \(\iff\) una fórmula es más restrictiva que la otra, o está incluida en la otra. \\
En el mundo de la lógica, decimos que A es más fuerte que B \(\iff A \implies B\)

\begin{itemize}
    \item Si (\(A \implies B\)) y (\(B \implies A\)) son tautologías, entonces A y B son equivalentes.
    \item Si (\(A \implies B\)) es tautología y (\(B \notimplies A\)) no es tautología, entonces decimos que A es más fuerte que B.
    \item Si (\(A \notimplies B\)) y (\(B \notimplies A\)) son contigencias, entonces no existe relación de fuerza entre A y B.
\end{itemize}

Algunos ejemplos:

\begin{itemize}
    \item \(\longitud{s} = 0 \implies \longitud{s} \ge 0\). En este caso vemos que \(\longitud{s} = 0\) es más fuerte que \(\longitud{s} \ge 0\) pues \(\longitud{s} = 0\) está incluido en \(\longitud{s} \ge 0\). Por lo tanto, \(A \implies B\)
    \item \(\longitud{s} = 0 \implies \longitud{s} \ge 3\). En este caso vemos que \(\longitud{s} = 0\) no es más fuerte que \(\longitud{s} \ge 3\) pues \(\longitud{s} = 0\) no está incluido en \(\longitud{s} \ge 3\). Por lo tanto, \(A \notimplies B\)
    \item \(2 \le i < \longitud{s} \implies 1 \le i < \longitud{s}\). En este caso A \(\implies\) B, pues i = 2 está incluido en el rango de B. Por lo tanto, \(A \implies B\)
    \item \(0 \le i < \longitud{s} \implies 1 \le i < \longitud{s}\). En este caso A \(\notimplies\) B, pues el 0 de A no es parte de B. Por lo tanto, \(A \notimplies B\)
\end{itemize}

\subsection*{Tipos de parámetros en especificacion}
\begin{itemize}
    \item in: Solo nos interesa el valor de entrada de una variable. No la vamos a modificar. Ya están inicializados
    \item out: Donde se retornará el resultado. No nos importa el valor inicial ni tampoco determina nada en nuestra función.
    \item inout: Necesitamos el valor original aunque lo terminamos modificando y devolviendo.
\end{itemize}
\subsection*{Lógica trivaluada}
También llamada lógica secuencial porque se procesa de izquierda a derecha; Nos introduce los conceptos de \(\yLuego \ \oLuego \ \implicaLuego\) y el valor de indefinido \(\bot\).

Se termina de evaluar una expresión cuando se puede deducir el valor de verdad. \\


Considere \(x = \True \land y = \bot \land z = \False \)
\begin{itemize}
    \item \(x \oLuego y\): Como el \(\oLuego\) necesita uno solo para ser verdadero, entonces como x ya es \(\True\) entonces toda la fórmula es verdadera.
    \item \(x \yLuego y\): Como el \(\yLuego\) necesita que ambas variables sean verdaderas, evalúa indefinido y el programa estalla.
    \item \(\neg x \implicaLuego y\): Como el \(\implicaLuego\) solo es falso si el antecedente es  \(\True\) y el consecuente \(\False\), como en este caso el antecedente ya es falso, toda la implicación es verdadera.
    \item \((x \land z) \yLuego y\): Como el \(\yLuego\) necesita que ambas fórmulas sean \(\True\), en este caso, como \((x \land z)\) es falso, entonces ya toda la fórmula es falsa. Nótese que el \(\land\) de la condición interna no contiene el luego porque jamás se indefinirá.
    \item \((\forall i: \ent)(0 \le i < \longitud{s} \implicaLuego s[i] \ge 0)\) Nótese que aquí usamos un \(\implicaLuego\) porque podría ser que la lista esté indefinida o no exista el valor en s[i]
\end{itemize}

\subsection*{Predicados}
\begin{itemize}
    \item Viven en el mundo de la lógica. 
    \item Nos sirven para poder modularizar nuestras especificaciones. 
    \item Solamente devuelven valores de verdad True y False y es necesario castearlos en caso de querer devolver bool como tipo de dato.
    \item Los predicados pueden llamar a otros predicados o funciones auxiliares.
    \item Pueden utilizar cuantificadores.
    \item No tienen requiere ni asegura.
    \item No admite parámetros in, out, inout.
\end{itemize}

\leavevmode
\\
Ejemplo cuando tenemos que transformar el valor de verdad a tipo de dato: 
\leavevmode
\\
\pred{divisiblePorDos}{n: \ent}{
    n \ mod \ 2 = 0
}

\begin{proc}{esMultiploDeDos}{\In n: \ent}{\bool}
    \requiere{\True}
    \asegura{res = true \iff divisiblePorDos(n)}
\end{proc}
\leavevmode
\\ 
Ejemplo usando un predicado sin necesidad de transformar el valor de verdad a tipo de dato: 

\leavevmode
\pred{todosSonPares}{l: \TLista{\ent}}{
    \paraTodo[unalinea]{i}{\ent}{0 \le i < \longitud{l}  \implicaLuego l[i] mod 2 = 0}
}

\begin{proc}{todosPares}{\In l: \TLista{\ent}}{\bool}
    \requiere{todosSonPares(l)}
\end{proc}
\newpage
\subsection*{Funciones Auxiliares}
\begin{itemize}
    \item Son reemplazos sintácticos. 
    \item Nos ayudan a modularizar las especificaciones.
    \item No pueden ser recursivas.
    \item Solo hacen cuentas.
    \item No pueden utilizar cuantificadores.
    \item Pueden llamar a predicados.
    \item Devuelven un tipo de dato.
    \item No tienen requiere ni asegura.
    \item No admite parámetros in, out, inout.
\end{itemize}

\leavevmode
\aux{sumar}{n: \ent, m: \ent}{\ent}{
    n + m
}

\aux{sumarTodos}{s: \TLista{\ent}}{\ent}{
    \sum_{i=0}^{\longitud{s}-1}{s[i]}
}


\subsection*{Aridad}
Decimos que una función es de aridad \(n\) cuando la función recibe \(n\) cantidad de parámetros.

\subsection*{Variables Ligadas y Libres}
Las variables son ligadas \(\iff\) están dentro de un cuantificador mientras que son libres cuando no lo están.
\begin{itemize}
    \item \((\forall i: \ent)(0 \le i < \longitud{s} \implicaLuego n \ge s[i])\) i es una variable ligada mientras que n y s son variables libres.
    \item \((\exists j: \ent)(0 \le j < \longitud{s} \yLuego n \ge s[i])\) j es una variable ligada mientras que n y s son variables libres.
    \item \((\forall i: \ent)(0 \le i < \longitud{s} \implicaLuego n \ge s[i]) \land P(i)\) Ojo acá. i es una variable ligada, pero la i que está fuera del cuantificador \(P(i)\) no está ligada. Esta última debería ser renombrada para no tener problemas y confusiones.
\end{itemize}

Cuando tenemos variables ligadas \textbf{no} podemos hacer nada sobre ellas, entre esas cosas, no podemos reemplazarlas porque no dependen de nosotros sino de los cuantificadores.

\subsection*{Cuantificadores anidados}
Anidamos cuantificadores cuando el rango de las variables es exactamente el mismo.
\begin{itemize}
    \item \((\forall i, j: \ent)(0 \le i, j < \longitud{s} \implicaLuego n \ge s[i][j]) \equiv (\forall i: \ent)((0 \le i < \longitud{s}\implicaLuego (\forall  j: \ent)(0 \le j < \longitud{s} \implicaLuego n \ge s[i][j]))) \) 
\end{itemize}

\subsection*{Estado}
Llamamos estado a los valores de las variables en un punto de ejecución específico. El estado de un programa es importante porque muta al asignar valores a las variables.
Cuando necesitamos hablar del estado de una variable en un instante específico, hablamos de \textbf{metavariables} \\ 

\subsection*{Metavariables}
Llamamos metavariable a una variable en un instante dado. Es útil cuando tenemos que predicar como cambio el valor de una variable con respecto al inicial. \\
Cuando tenemos que utilizar metavariables, sea S una variable cualquiera podemos referirnos al instante de tiempo de S como \(S_{t}\) donde t indica el momento. \\ \\ 
Notación \(S = S_{0}\)

\begin{proc}{multiplicarPorDosAImpares}{\Inout l: \TLista{\ent}}{}
    \requiere{l = l_{0}}
    \asegura{\longitud{l} = \longitud{l_{0}}}
    \asegura{(\forall i: \ent)(0 \le i < \longitud{s} \implicaLuego if(s_{0} [i] \ mod \ 2 \neq 0) \ then \ (s[i] = s_{0}[i] \ast 2) \ else \ (s[i] = s_{0}[i]) \ fi)}
\end{proc}
\leavevmode
\\
Nota: Cuando utilizamos metavariables tenemos que indicar que al modificar algo directamente, si no modificamos todo el conjunto de valores tenemos que indicar que los demás permanecen inalterados. En este caso, como estamos editando los valores, no tendría sentido que la lista salga con mayor longitud, es por eso que garantizamos que no cambia. 

Otra manera de resolver el ejemplo anterior es utilizando old(s)
\begin{proc}{multiplicarPorDosAImpares}{\Inout l: \TLista{\ent}}{}
    \asegura{\longitud{l} = \longitud{old(l)}}
    \asegura{(\forall i: \ent)(0 \le i < \longitud{s} \implicaLuego if(old(s)[i] \ mod \ 2 \neq 0) \ then \ (s[i] = old(s)[i] \ast 2) \ else \ (s[i] = old(s)[i]) \ fi)}
\end{proc}
\leavevmode
\\

\section*{Correctitud de un Programa}
Decimos que un programa S es correcto respecto a una especificación si se cumple la precondición P, el programa termina su ejecución y se cumple la postcondición Q.
\subsection*{Tripla de Hoare}
Notación para indicar que S es correcto respecto a la especificación (P, Q) \\ 
\[\{P\} \ S \ \{Q\}\]

\subsection*{SmallLang}
Es un lenguaje que nos permitirá poder validar la correctitud de un programa.
Solo tiene dos operaciones: 
\begin{itemize}
    \item x := E \(\equiv\) asignación
    \item skip \(\equiv\) no hace nada
\end{itemize}

Nota: E es una expresión cualquiera. Un valor, una función, cualquier cosa.

\subsection*{Estructuras de Control en SmallLang}
\begin{itemize}
    \item Secuencia de pasos: S1; S2 es un programa \(\iff\) S1 y S2 son dos programas.
    \item Condicionales: if B then S1 else S2 endif es un programa \(\iff\) B es una condición lógica (guarda) y S1 y S2 son programas.
    \item Ciclo: while B do S endwhile es un programa \(\iff\) B es una condición lógica y S un programa.
\end{itemize}

\subsection*{Validez de una tripla de Hoare}
\(\{x \ge 4\} \ x := x+1 \ \{x \ge 7\}\)
Donde, 
\begin{itemize}
    \item P = \(\{x \ge 4\}\)
    \item S = \(x := x+1\)
    \item Q = \(\{x \ge 7\}\)
\end{itemize}

¿Vale que \(\{P\} \ S \ \{Q\}\)?
Solo vale \(\iff x \ge 6\) por lo tanto, como la precondición P falla en los casos de x = 4, x = 5 podemos decir que la tripla de Hoare no es válida.\\

Esto que acabamos de hacer se llama demostrar la correctitud de un programa, y acabamos de demostrar que la precondición P para el programa S es demasiado débil pues no nos garantiza que llegaremos a Q cumpliendo P. \\

Existe una manera formal que nos permite conocer la precondición más débil de un algoritmo.
\newpage
\subsection*{Predicado def(E)}
Dada una expresión E, llamamos def(E) a las condiciones para que E esté definida. Todas las constantes están definidas, por lo tanto def(x) \(\equiv\) True. La idea es ir separando en términos e ir colocando las definiciones necesarias para esa operación específica.
\begin{itemize}
    \item \(def(x+1) \equiv def(x) \land def(1) \equiv True \land True \equiv True\)
    \item \(def(x/y) \equiv def(x) \land (def(y) \land y\neq0) \equiv True \land (True \land y>0) \equiv y\neq0\)
    \item \(def(\sqrt{x}) \equiv (def(x) \land x\ge0)\)
    \item \(def(a[i]+3) \equiv (def(a) \land def(i)) \yLuego 0 \le i<\longitud{a} \land def(3) \equiv (True \land True) \yLuego 0\le i<\longitud{a} \land True \equiv 0\le i<\longitud{a}\)
\end{itemize}

\subsection*{Predicado \(Q^{x}_{E}\)}
Cuando hablamos de este predicado hablamos de reemplazar las ocurrencias de x por E en el programa. Solo se reemplazan las ocurrencias libres, no las ligadas.
\subsection*{Axiomas}
\begin{itemize}
    \item Axioma 1: \(wp(x := E, Q) \equiv def(E) \yLuego Q^{x}_{E}\)
    \item Axioma 2: \(wp(skip, Q) \equiv Q\)
    \item Axioma 3: \(wp(S1; S2, Q) \equiv wp(S_{1}, wp(S_{2}, Q))\)
    \item Axioma 4: \(wp(S, Q) \equiv def(B) \yLuego ((B \land wp(S_{1}, Q)) \lor (\neg B \land wp(S_{2}, Q)))\)
\end{itemize} 

\subsection*{Axioma 1 con secuencias}
El axioma 1 nos sirve para asignar una expresión a una variable; Sin embargo, si tenemos que guardar algo en una secuencia debemos utilizar el setAt.
\begin{itemize}
    \item \(wp(b[i]:=E, Q) \\
    \equiv def(setAt(b, i, E)) \yLuego Q^{b[i]}_{setAt(b, i, E)} \\
    \equiv (def(b) \land def(i) \land def(E)) \yLuego 0\le i < \longitud{b} \yLuego Q^{b[i]}_{setAt(b, i, E)} \\
    \equiv 0\le i < \longitud{b} \yLuego Q^{b[i]}_{setAt(b, i, E)}   \\
    \equiv setAt(b, i, E)[j] = \{E \ si \ i = j, b[j] \ si \ i \neq j\}\)
\end{itemize}

Algunos ejemplos:\\
\(wp(s[i]:=s[i-1], Q) \\
wp(setAt(s, i, s[i-1]), Q) \equiv \\
def(setAt(s, i, s[i-1])) \equiv (def(s) \land def(i)) \yLuego 0 \le i < \longitud{s} \yLuego def(s[i-1]) \equiv \\ 0\le i < \longitud{s} \yLuego (def(s) \land def(i)) \yLuego 0\le i-1 < \longitud{s} \equiv  0\le i < \longitud{s} \yLuego 1\le i < \longitud{s} + 1 \equiv  \textcolor{blue}{1 \le i < \longitud{s}}
\)


TODO: Luego mostrar un ejercicio y aclarar que por cada condición se separan n cuantificadores.

\subsection*{Precondición más débil (Weakest Precondition)}
Es la precondición más débil que se necesita para poder ejecutar un algoritmo y satisfacer la postcondición Q. \\ \\
Notación: \(wp(S, Q)\) donde S es el programa y Q la postcondición. \\ \\
Teorema: Una tripla de Hoare \(\{P\} \ S \ \{Q\}\) es válida \(\iff P \implicaLuego wp(S, Q)\). \\ \\ 
Sea el siguiente enunciado, calcule la precondición más debil.
\begin{itemize}
    \item P = \(\{x \ge 4\}\)
    \item S = \(x := x+1\)
    \item Q = \(\{x \ge 5\}\)
\end{itemize} 
\(P \implicaLuego wp(S, Q) \equiv wp(x := x+1, x \ge 5) \equiv def(x+1) \yLuego Q^{x}_{x+1} \equiv def(x) \yLuego def(1) \yLuego x+1 \ge 5 \\ \equiv True \yLuego True \yLuego x \ge 4 \equiv x \ge 4\)

Luego, \(\{x \ge 4\} \implicaLuego \{x \ge 4\}\) es \(\True\)

Por lo tanto, \(wp(x:=x+1, x \ge 5) \equiv \{x \ge 4\}\)

Finalmente, probamos que para poder satisfacer Q la precondición más debil que cumple P es cuaqluier \(x \ge 4\). \\

Nota importante: Muchas veces puede ser que se nos solicite indicar que \(wp\) es incorrecta. Cuando se dice esto, significa que son precondiciones válidas pero hay algunas que no son la más débil.

\newpage 
\section*{Precondición más débil en ciclos}
Consideremos el siguiente ejemplo
\(\{???\} S \{x=0\}\)
donde S es el siguiente programa 

\begin{lstlisting}
     while(x>0) do
        x := x-1
    endwhile
 \end{lstlisting}

 Recordemos que esto es una tripla de Hoare, pero no podemos utilizar wp(S, Q) porque el programa es en base a ciclos. La precondición P más débil acá sería que x\(\ge\)0 porque para cumplir la postcondición Q me da igual si entra al ciclo o no.
 Eso tiene que quedar siempre claro, cuando estamos hablando en ciclos, si no entra al ciclo y satisface igual ya está.
 \subsection*{Predicado \(H_{k}\)}
 Definimos el predicado \(H_{k}\) para poder controlar la cantidad de iteraciones que hace un ciclo y poder calcular la precondición más débil.
 \begin{itemize}
    \item \(H_{0}(Q) \equiv def(B) \land \neg B \land Q\)
    \item \(H_{k+1}(Q) \equiv def(B) \land B \land wp(S, H_{k}(Q)) \ para \ k \ge 0\) 
\end{itemize} 
\subsection*{Axiomas}
Definimos el Axioma 5 para poder verificar la correctitud de ciclos que hacen una cantidad de iteraciones fijas como \\
\[wp(while \ B \ do \ S \ endwhile, Q) \equiv (\exists_{i\ge0})(H_{i}(Q))\]

¿Cual es el problema del Axioma 5 y el Predicado \(H_{k}\)? El problema es que ambos nos sirven para poder validar la precondición de un ciclo que sabemos que finaliza luego de \(n\) iteraciones.

\subsection*{Predicado \(I\) - Invariante}
Definimos el Invariante de un ciclo como un predicado que:
\begin{itemize}
    \item Demuestra que el ciclo realmente funciona y nos ayuda a demostrar la correctitud parcial "si termina el ciclo... entonces se cumple Q"
    \item Vale antes de entrar al ciclo y luego de salir del ciclo.
    \item En cada iteración, el invariante debe volver a ser válido. En el medio de la iteración puede que deje de valer.
    \item Un buen invariante incluye el rango de la(s) variable(s) de control del ciclo.
    \item Un buen invariante tiene alguna afirmación sobre el acumulador del ciclo.
\end{itemize} 

\subsection*{Teorema del Invariante}
Para poder probar que un ciclo es correcto, usamos el teorema del invariante.
\begin{itemize}
    \item \(P \equiv \) Las condiciones del requiere
    \item \(P_{c} \equiv\) Las precondiciones que necesita el ciclo para poder ingresar, muchas veces son las líneas que están por encima del while y el requiere (¡no todas las del requiere!)
    \item \(I \equiv \) Las condiciones que suceden antes y después de entrar al ciclo
    \item \(B \equiv \) La guarda del ciclo
    \item \(Q_{c} \equiv\) La postcondición del ciclo
  
\end{itemize} 
Luego, 
\begin{itemize}
    \item \(\ \{P\} \ S \ \{P_{c}\}\)
    \item \(\ P_{c} \implies I\)
    \item \(\{I \land B\} \ S \ \{I\}\)
    \item \(\{I \land \neg B\} \implies Q_{c}\)
\end{itemize} 
\textbf{Recordatorio}: Si aparece la S es que tenemos que calcular la wp porque es parte de la Tripla de Hoare. \\
\textbf{Recordatorio importante}: Para todo lo que es wp usamos SmallLang, es decir, cuando tenemos que validar la tripla de Hoare. En todos los demás casos usamos lógica. \\
Ej: No sería válido tener un if en un invariante.
\subsection*{Teorema de Terminación de Ciclo}
Utilizamos el teorema de terminación de ciclo para garantizar que cumpliendo la precondición de un ciclo, el programa siempre termina.
Para poder probar esto, necesito una función variante \(f_{v}\). \\
Llamamos función variante \(f_{v}\) a una función que es siempre estrictamente decreciente y representa una cantidad que se va reduciendo a lo largo de las iteraciones.
La función \(f_{v}\) debe garantizar
\begin{itemize}
    \item \(\{I \land B \land v_{0} = f_{v}\} \ S \ \{f_{v} < v_{0}\}\)
    \item \(\{I  \land f_{v} \le 0 \implies \neg B\}\)
   
\end{itemize}
\subsection*{Reglas generales para validar correctitud de ciclo y terminación}
\begin{itemize}
    \item Cuando tenemos que iterar sobre listas, tendremos un índice i que irá hasta incluida la longitud (pues nos sirve) para negar la guarda, mientras que el j será para usar dentro del ciclo. 
    \((0\le i \le \longitud{s} \yLuego (0 \le j < i \implicaLuego res = \sum_{j=0}^{i}{s[j]}))\)
    \item Si tengo algo en \(I\) deberá estar en \(P_{c}\) y/o \(Q_{c}\). Esto es porque cuando tengamos que hacer las implicancias, deberíamos comparar de ambos lados y si el invariante tiene algo que los demás no, es siempre falso. 
    \item Si tengo algo en \(P_{c}\) o \(Q_{c}\) no necesariamente tiene que estar en \(I\)
    \item En \(Q_{c}\) se acostumbra a colocar que \(i = \longitud{s}\) porque nos ayuda a demostrar la terminación del ciclo.
    \item En la función variante: i va negada si i crece en el ciclo; i va positivo si i decrece en el ciclo.
    \item En \(Q_{c}\) rara vez tenemos que hablar de rangos de variables.
\end{itemize} 

\section*{TADs (Tipos Abstractos de Datos)}
Es un tipo de datos porque define un conjunto de valores y las operaciones que se pueden realizar sobre ellos. Es abstracto ya que para utilizarlos no necesitamos saber como está implementado.
Describe el qué y no el cómo.
Son una forma de modularizar a nivel de los datos.  \\ \\
Importante: En todo enunciado ambiguo, queda en nosotros interpretarlo y eliminar esas ambiguedades decidiendo como lo vamos a considerar.\\ \\
Ej: En varios ejercicios te piden que un punto deba cambiarse el centro y hay dos maneras de plantearlo
\begin{itemize}
    \item Agarrar el centro que vienen por parámetro y sumar coordenada a coordenada con respecto a las de la instancia de mi TAD. 
    \item Agarrar el centro que viene por parámetro y considerarlo como el centro final en la instancia del TAD.
\end{itemize}
Importante: En un TAD podemos usar todos los mismos tipos de datos de especificación y sus funciones correspondientes.

\subsection*{Observadores}
Los observadores son una especie de atributo en un objeto en POO.
Nos permiten saber qué valores tienen y en qué momento.
\begin{itemize}
    \item Sirven para describir el estado de una instancia de un TAD y nos permiten consultar su estado virtual.
    \item Tenemos que poder observar todas las características que nos interesan de las instancias.
    \item En un instante de tiempo, el estado de una instancia del TAD estará dado por el estado de todos sus observadores (como un debugger).
    \item Nos permiten distinguir si dos instancias son distintas.
    \item Todas las operaciones tienen que poder ser descriptas a partir de los observadores.
\end{itemize} 
Ejemplo de un TAD con observadores de tipo dato:
\begin{lstlisting}
    TAD lista {
        obs elems: conj<T>
        obs cantElems: int
    }
\end{lstlisting}

Los observadores también pueden ser funciones auxiliares.
\begin{itemize}
    \item Son las auxiliares de nuestro lenguaje de especificación 
    \item No pueden tener efectos colaterales ni modificar los parámetros
    \item Pueden usar tipos de nuestro lenguaje de especificación 
\end{itemize} 
Ejemplo de un TAD con observadores de tipo dato:
\begin{lstlisting}
    TAD lista {
        obs elems: conj<T>
        obs estaElem(e: T): bool
    }
\end{lstlisting}

\subsection*{Igualdad Observacional}
Decimos que dos TADs son iguales \(\iff\) todos sus observadores son iguales.\\
Muchas veces no nos basta con la igualdad por defecto con los observadores, y tenemos que declarar nuestra propia igualdad observacional.

\subsection*{Operaciones de un TAD}
Las operaciones de un TAD deben estar especificadas y nos indican qué se puede hacer con una instancia.
Antes y una vez aplicada una operación tenemos que hablar del estado en el que quedó el TAD con respecto a sus observadores.
\section*{Implementación de un TAD}
\subsection*{Módulos}
Los módulos son la implementación de un TAD. Los módulos implementan el TAD.
\subsection*{Variables de Estado}
Lo que en los TADs eran observadores, acá son variables de estado. Serán manipuladas por las operaciones mediante el código de los algoritmos. 
Tipos válidos para variables de estado: 
\begin{itemize}
    \item int, real, bool, char, string
    \item tupla\(<\)T1, T2, T3\(>\), struct\(<\)campo1: val1, campo2: val2\(>\)
    \item Array\(<\)T\(>\) (arrays de tamaño fijo)
    \item No es posible usar tipos de especificación como conj\(<\)T\(>\) o seq\(<\)T\(>\)
\end{itemize}
El módulo puede tener variables de estado que no hagan referencia a ningún observador de un TAD, por ejemplo: guardar un máximo en un módulo aunque el TAD no lo pida. A veces estas cosas se hacen solo por temas de a la hora de implementarlo en un lenguaje de programación.
\subsection*{Invariante de Representación}
\begin{itemize}
    \item Define una restricción sobre el conjunto de valores que pueden tomar las variables de estado para que se considere una instancia válida.
    \item Es equivalente al invariante en ciclos, pero acá aplicado a TAD's
    \item Este es inicializado en el constructor una vez realizada la instancia
    \item Vale siempre antes de llamar a los métodos y luego de cada método. Siempre y cuando tenemos un parámetro inout, tenemos que ver que los cambios que hagamos hagan seguir valiendo a la invariante de representación.
    \item Todos los métodos pueden asumir que el invariante de representación siempre vale al ser llamados.
    \item ¡Prestar atencion a las verificaciones bidireccionales! En el sentido de que si tengo \(dict<Alarma, Conj<Sensores>>\) y \(dict<Sensor, Conj<Alarmas>>\) si en \(dict<Alarma, ...>\) tengo varios sensores, esos sensores estan en Sensor y ademas, si busco en las alarmas de cada sensor deberia estar la \(dict<Alarma, ...>\)
    \item Cuando tengo un diccionario tengo que usar un cuantificador para hablar y tengo que acceder y hablar si o si por la key.
\end{itemize}
\subsection*{Función de Abstracción}
Definimos función de abstracción (\(\alpha\)) como la representación física de lo que tiene el código, es decir, cuando acá hablamos de instancia.elems decimos que el código tiene un campo llamado elems. En TAD cuando nosotros hablabamos de un observador podía existir o no en la implementación como atributo pero seguir cumpliendo la especificación. \\
\begin{itemize}
    \item Es un predicado
    \item Toma como parámetro una instancia del módulo y una instancia del TAD.
    \item Hacemos referencia a observadores y a otros predicados, no procs
    \item Todos los observadores del TAD deben tener un vinculo con una variable de estado pero no necesariamente al revés
    \item Para poder escribir, usamos lenguaje de especificación como lo conocemos. Podemos usar por ejemplo |a.data| o a.data.length da igual, pero hay que ser consistentes
    \item Rara vez se colocan rangos acá, casi nunca, excepto cuando hay alguna variable de estado y observadora que hablan de capacidades.
\end{itemize}
Es importante considerar que todo lo que está en el invariante de representación ya está implícito en la función de abstracción. \\
Ej: Tenemos un módulo que tiene ventas, mayorProductoVentas, y mayorPrecio pero el TAD solo tiene ventas. En este caso, en el invariante de representación hablamos de mayorProductoVentas y mayorPrecio haciendo las relaciones con ventas (en temas de key del producto) pero en el predicado de astracción NO hablás de mayorProductoVentas y mayorPrecio primero porque no son observadores del TAD pero tampoco hay que garantizar nada porque ya está implicado por el invariante de representación

\subsection*{Operaciones de un Módulo}
Como un módulo implementa un TAD, todas sus operaciones deben estar implementados en código.
Para escribir las implementaciones de los procs usamos una especie de SmallLang.
\begin{itemize}
    \item Declaración de variables: 
    \begin{itemize}
        \item var x: int
        \item var c: ConjuntoArr<int>
    \end{itemize}
    \item Asignación: x := valor
    \item Condicional: if condicion then codigo else codigo endif
    \item Ciclo: while condicion do codigo endwhile
    \item Llamar a un proc:
    \begin{itemize}
        \item c.vacio()
        \item b : = c.vacio();
    \end{itemize}
\end{itemize}
\subsection*{Memoria dinámica}
Los tipos complejos son usados siempre por referencia. El valor indefinido es identificado por la palabra null. Acceder a algo null explota. \\
Las variables de tipos complejos deben ser inicializadas mediante el operador new.
Tipos nativos: 
\begin{lstlisting}
   var a: Array<int>
   var b: int
   if a == null then 
   ...codigo
   endif
   a := new Array<int>(10)
   b := a[0]
\end{lstlisting}
Tipos de otros módulos: 
\begin{lstlisting}
    var a: ConjArr<int>
    a := new ConjArr(10) //longitud 10
\end{lstlisting}
Pasaje por referencia:
\begin{lstlisting}
    var a: ConjArr<int>
    var b: ConjArr<int>
    a := new ConjArr(10); //longitud 10
    b := a
\end{lstlisting}
Nota: El pasaje por referencia, a veces se le conoce comúnmente como aliasing y para evitarlo, hay que crear una nueva instancia con los valores de la otra instancia en vez de asignarla directamente.

\subsection*{Contrato entre métodos de un módulo}
Los métodos de un Módulo tienen un contrato entre sí.
Supongamos que necesitamos que la complejidad de búsqueda de un método de mi módulo sea o(log n) sabiendo que mi lista está ordenada. Aquí, el método busqueda podrá usar la búsqueda binaria para poder hacer el proceso lo más rápido posible, pero ¿qué sucede si al agregar un elemento en el método agregar() la lista de salida que se modifica en el módulo no está ordenada? \\ El contrato de búsqueda no se cumpliría porque la búsqueda binaria no funcionaría porque necesita que la lista esté ordenada. \\

En este caso, que la lista esté ordenada debería ser una condición del invariante de representación para evitar estos problemas. \\

Este módulo es bueno para la búsqueda de un elemento pero el agregar un elemento es mucho más costoso por el tema de reordenar la lista nuevamente. \\

\textbf{Mirada al futuro}: Los métodos deben cumplir una complejidad algorítimica dada.
\section*{Estructuras de Datos}
\subsection*{Colección}
Representa un grupo de objetos. Provee de una arquitectura para su almacenamiento y manipulación.
\begin{itemize}
    \item Secuencia
    \item Conjunto
    \item Multiconjunto
    \item Diccionario
\end{itemize}
¿Por qué una tupla no es parte del grupo? Porque la tupla es fija, una vez definida e inicializada su longitud no cambia.
\subsection*{Tipos paramétricos}
Son variables de tipo, es decir, variables con tipo genérico.
La manera más común de indicar un genérico son T, K o V.  \\
Hay que tener cuidado con las operaciones que se realizan con las variables genéricas porque algunos operadores nos restringuen su uso a ciertos tipos. \\
Ej: No puedo utilizar X - Y si X, Y son genéricos y nos envían X, Y como char.
\subsection*{Iteradores}
Nos permiten recorrer colecciones de una manera abstracta sin saber su estructura. \\
Un buen iterador se distingue de otro iterador por la velocidad de iterar la estructura. \\
Operaciones con iteradores:
\begin{itemize}
    \item ¿Estoy sobre un elemento? 
    \item Obtener el elemento actual
    \item Avanzar al siguiente elemento
    \item Retroceder al elemento anterior (sii es bidireccional)
\end{itemize}
Los Iteradores son una clase privada que van dentro de la clase que queremos que se instancie y se pueda recorrer. \\
Es privada porque no queremos que sea instanciada más que por la instancia de la clase que queremos recorrer. \\
Los Iteradores no van hasta n, van hasta n+1.
\begin{figure}[h]
    \includegraphics[]{assets/iterador.png}
\end{figure}
\begin{lstlisting}
    public class Vector<T> implements List<T>{
        private T[] elementos;
        private int size;
        
        private class Iterador implements Iterator<T>{
            int indice;
            
            Iterador(){
                indice = 0;
            }
            public boolean hasNext(){
                return indice != size;
            }
            public T next(){
                int i = indice;
                indice = indice + 1;
                return elementos[i];
            }
        }

        public Iterator<T> iterator(){
            return new Iterador();
        }
    }

    Iterator it = vector.iterator();
    while(it.hasNext()){
        System.out.println(it.next());
    }
\end{lstlisting}

\subsection*{Singly Linked Lists - Listas simplemente enlazadas}
Es una estructura que sirve para representar una secuencia de elementos donde cada elemento es un Nodo que tiene un valor y una referencia al siguiente. \\ 
Nota: Si el Nodo actual es el último de la Singly Linked List lo notamos porque el ultimo.siguiente = null.
El Nodo debe ser responsable de guardar al siguiente porque caso contrario, no existirá otra referencia al siguiente Nodo.
Ventajas de las Singly Linked Lists:
\begin{itemize}
    \item Mas fino uso de la memoria.
    \item Insertamos fácilmente al principio y al final.
    \item El costo de insertar al final es o(n) pues debemos recorrer todos los nodos para insertar uno nuevo. Es por eso que es común guardar el último nodo almacenado en la clase.
    \item Eficiente para reacomodar elementos.
    \item Es malo en rendimiento o(n) si necesito buscar un elemento en específico. Es decir, perdemos el acceso aleatorio a los elementos.
\end{itemize}
\subsection*{Double Linked Lists - Listas doblemente enlazadas}
Exactamente igual que la Singly Linked Lists pero acá los Nodos tienen almacenado: Referencia al nodo previo, valor y referencia al nodo siguiente. 
Nota: Si el Nodo actual es el último de la Double Linked List lo notamos porque el ultimo.siguiente = null.
\subsection*{Listas vs Linked Lists}
Ambas son bastante rápidas a la hora de iterar sobre ellas, sin embargo: 
\begin{itemize}
    \item Las listas nos permiten acceder rápidamente a un elemento mientras que las Linked Lists no.
    \item Las Linked Lists nos permiten agregar elementos rápidamente al inicio (O(1)) y no necesitamos reacomodar nada más que decir que el anterior primero ahora es el segundo mientras que en la lista tenemos que reacomodar todos los elementos como antes pero colocar el nuevo al principio (O(n))
\end{itemize}

\section*{Complejidad Algorítmica}
\subsection*{Analisis de la Complejidad de Algoritmos}
Nos permite elegir entre distintos algoritmos para resolver el mismo problema o distintas formas de implementar un TAD.
Esto es importante porque nos permite optimizar:
\begin{itemize}
    \item Tiempo de ejecución
    \item Espacio (memoria)
    \item Cantidad de procesadores (en caso de algoritmos paralelos)
    \item Utilización de la red de comunicaciones (para algoritmos paralelos)
\end{itemize}

El análisis se puede hacer de forma experimental o teórica \\

\textbf{Ventajas del enfoque teórico}:
\begin{itemize}
    \item Se hace antes de escribir código
    \item Vale para todas las instancias del problema (no en un caso específico)
    \item Es independiente del lenguaje de programación
    \item Es independiente de la máquina donde se ejecuta (el tiempo no varía)
    \item Es independiente del programador
\end{itemize}

\subsection*{Análisis Teórico}
\begin{itemize}
    \item Se realiza en función del tamaño del input 
    \item Para distintos tipos de input 
    \item Análisis asintótico
\end{itemize}

\subsection*{Operaciones Elementales}
T(l) será una función que mida el número de operaciones elementales requeridas para la instancia l.
Las operaciones elementales (OE) serán aquellas que el procesador realiza en tiempo acotado por una constante (no depende del tamaño de la entrada).
Ej: x = 0, if(x = 2).
Es decir, podemos generalizar las operaciones elementales a:
\begin{itemize}
    \item Operaciones aritméticas básicas (suma, resta, división, multiplicación)
    \item Comparaciones y/o operaciones lógicas
    \item Acceder a elemento de array
    \item Asignaciones a variables de tipos básicos (las inicializaciones no consumen tiempo)
\end{itemize}
\subsection*{Cálculo de Operaciones Elementales}
\begin{itemize}
    \item T(If C Then S1 Else S2 Endif;) = T(C) + max\{t(S1), t(S2)\}
    \item T(Case C Of v1:S1 \(|\) v2:S2 \(|\) ... \(|\) vn:Sn End;) = T(C) + max\{t(S1), t(S2), ..., T(Sn)\}
    \item T(While C Do S End;) = T(c) + (nº iteraciones) * (T(S) + T(C))
    \item T(MiFuncion(P1, P2, ..., Pn)) = 1 + T(P1) + T(P2) + ... + T(Pn)
\end{itemize}
\subsection*{Tamaño de la entrada}
\begin{itemize}
    \item T(n): complejidad temporal (o en tiempo) para una entrada de tamaño n
    \item S(n): complejidad espacial para una entrada de tamaño n
\end{itemize}
\textbf{Importante}: NUNCA hay que restringir la longitud de entrada de una lista; Por ejemplo: no tiene decir que el mejor caso sea que la lista esté vacía, porque la complejidad se da en base a listas de longitud n sin ningún tipo de restricción.
\subsection*{Análisis de los casos}
\begin{itemize}
    \item T\(_{mejor}(n)\) = min\(_{instancias \ l},\ _{\longitud{l} \ = \ n}\ \) \{t(l)\}. Recorriendo una lista, el mejor caso es que esté en primera posición.
    \item T\(_{peor}(n)\) = max\(_{instancias \ l},\ _{\longitud{l} \ = \ n}\ \) \{t(l)\}. Recorriendo una lista, el peor caso es que el elemento no esté.
    \item T\(_{prom}(n)\) = No lo vamos a usar pero, es algo más parecido a estadística.
\end{itemize}
Cuando el tamaño de datos es grande, los costos de los diferentes algoritmos pueden variar de manera significativa. En tamaño de datos chico, no nos interesa el tiempo de ejecución.
\subsection*{Principio de Invarianza}
Si dos algoritmos solo varían en una constante, entonces no nos importa porque la complejidad es la misma.
\[T_{1}(n) \le cT_{2}(n)\]
c: constante real  \(c>0\) \\
\(n_{0} \in N\) tales que \(\forall n \ge n_{0}\)
\subsection*{Comportamiento Asintótico}
Comportamiento para los valores de la entrada suficientemente grandes. \\
Nota: Las funciones f y g dependen de un n. No existen funciones constantes menores que O(1).
\subsection*{O (cota superior)}
Sirve para representar el límite o cota superior del tiempo de ejecución de un algoritmo. \\
La notación f \(\in\) O(g) expresa que la función f no crece más rápido que alguna función proporcional a g (g es la cota superior de f). \\
Ej: 100\(n^{2} + 300n + 1000 \in O(n^{2})\) \(\land\) 100\(n^{3} + 300n + 1000 \in O(n^{2})\) \\ \\

\[ f (n) \in O(g(n)) \iff \exists c \in R>0, n_{0} \in N \ tal \ que \
\forall n \ge n_{0} : f (n) \le c \ast g(n) \]

Las funciones f son aquellas que crecen más lento, ej si me dan O(n) busco funciones que crezcan mas lento que n
\begin{itemize}
    \item \(n-1 \in O(n) \) porque al ser -1 una constante, no cambia nada.
    \item \(2n \in O(n)\) porque al ser dos veces n (n+n) y es una constante, no cambia nada.
    \item \(n+1 \in O(n)\)  porque al ser +1 una constante, no cambia nada.
    \item \(n\ log(n) \in O(n)\)
    \item \(log(n) \in O(n)\)
    \item \(n^{2} \notin O(n)\)
\end{itemize}
\[\begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\linewidth]{assets/notacion_o.png}
\end{minipage}\]
\subsection*{\(\Omega\) (cota inferior)}
Sirve para representar el límite o cota inferior del tiempo de ejecución de un algoritmo.
La notación f \(\in \Omega(g)\) expresa que la función f está acotada inferiormente por alguna función proporcional a g (g es cota inferior de f). \\
Ej: 100\(n^{2} + 300n + 1000 \in Omega(n^{2})\) \(\land\) 100\(n^{2} + 300n + 1000 \in Omega(n)\)
\begin{itemize}
    \item \(n/2 \in \Omega(n)\)
    \item \(n-1 \in \Omega(n)\)
    \item \(n^{2} \in \Omega(n)\)
    \item \(n^{k} \in \Omega(n)\) 
    \item \(log(n) \notin \Omega(n)\) 
\end{itemize}
\[\begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\linewidth]{assets/notacion_omega.png}
\end{minipage}\]
\subsection*{\(\theta\) (orden exacto)}
\[f (n) \in \theta(g(n)) \iff f (n) \in O(g(n)) \ y \ f (n) \in \Omega(g(n)).\ Es \ decir, \ \theta(g(n)) = O(g(n)) \cap \Omega(g(n))\]
Básicamente: Tiene que valer en O y en \(\Omega\), no debe pasarse de \(\Omega\) pero tampoco estar por debajo de O.
\[\begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\linewidth]{assets/notacion_theta.png}
\end{minipage}\]
\subsection*{Definición parecida a inducción corrida en Complejidad}
Tanto en la definición de \(O, \ \Omega \ y \ \theta\) se nombra para un \(n > n_{0}\). Esto es porque no todas las funciones cumplirán la definición, sino aquellas que son mayores a \(n_{0}\). Es por eso que en las imágenes se ven como inicialmente no cumple, pero a partir de un \(x_{0} \ (n_{0})\) vale para todo n.
\subsection*{Propiedades}
\begin{itemize}
    \item Suma: O(f) + O(g) = O(f+g) = O(max{f,g})
    \item Producto: O(f) \(\ast\) O(g) = O(f*g)
    \item Reflexividad: f \(\in O(f)\)
    \item Simetría: Solo vale en \(\theta\) = \(f \in \theta(g) \implies g \in \theta(f)\)
    \item Transitividad: \(f \in O(g) \land g \in O(h) \implies f \in O(h)\)
\end{itemize}
\subsection*{Intuición de Análisis de Complejidad posibles}
\begin{itemize}
    \item Mejor caso != Peor caso (cuando el ciclo corta antes)
    \item Mejor caso = Peor caso (cuando el ciclo no tiene condición de corte)
    \item Cuando tengo ramas if else, la complejidad mayor se dará por la rama que tenga mayor cantidad de operaciones en cuanto a ciclos.
    \item Cuando tenga guardas como por ejemplo, while b > 0 y la variable se va dividiendo entre dos, la complejidad será algo parecida a log n.
    \item Cuando una función recibe dos parámetros, pero uno de ellos no tiene un rol fundamental en algún ciclo o guarda, entonces raramente esté en el cálculo final de la complejidad.
    \item Recordar que es normal ver casos donde se aplique la sumatoria de gauss o tengamos que masajear la expresión para que aparezca.
\end{itemize}

\subsection*{Cota Ajustada}
Usamos \(\theta\) para indicar cotas ajustadas al momento de tener que calcular el peor y el mejor caso.
\subsection*{Complejidad Algorítmica en Polinomios}
Usamos límites para estos casos \\

Sean f, g: \(\mathds{N} \implies \float_{>0}\). Si existe:
\[ \lim_{n\to\infty} f(x)/g(n) = l \in \float_{\ge 0} \cup \{ +\infty \} \]
\begin{itemize}
    \item \(f \in \theta(g) \iff 0<l<+\infty\)
    \item \(f \in O(g) \ y \ f \notin \Omega(g) \iff l = 0\)
    \item \(f \in \Omega(g) \ y \ f \notin O(g) \iff l = +\infty\)
\end{itemize}

\subsection*{Ejemplo 1 en cálculo de complejidad}
\begin{lstlisting}
    void add(int mat1[n][n], mat2[n][n]){
        int ans[n][n];
            for(int i = 0; i < n; i++){
                for(int j = 0; j < n; j = j+1){
                    ans[i][j] = mat1[i][j] + mat2[i][j];
                }
            }
        return ans[0][0];
    }
\end{lstlisting}
$\textbf{Parte 1}$: Comenzamos colocando la cantidad de operaciones que se realizan, línea a línea. Los ciclos, en el caso del for, posee 3 casos: el caso inicial, caso intermedio, caso fin.
\begin{lstlisting}
    void add(int mat1[n][n], mat2[n][n]){
    0    int ans[n][n];
    2 2 2   for(int i = 0; i < n; i++){
    2 3 3       for(int j = 0; j < n; j = j+1){
    8               ans[i][j] = mat1[i][j] + mat2[i][j];
                }
            }
    3    return ans[0][0];
    }
\end{lstlisting}
Obs: j = j+1 $\neq$ j++ en temas de cantidad de operaciones. El j++ es una operación menos aunque hagan lo mismo.
Obs: El return funciona como una asignación, por lo tanto también cuenta en operaciones. \\
$\textbf{Parte 2}$: Elegimos si empezamos por el mejor, o el peor caso. El peor caso suele ser el más tedioso así que empecemos por ese. \\
$\textbf{Parte 2.1: Peor caso}$ \\
$\textbf{Parte 2.1.1 }$: Empezamos considerando el caso intermedio del ciclo.
\begin{lstlisting}
    2 3 3   for(int j = 0; j < n; j = j+1){
    8           ans[i][j] = mat1[i][j] + mat2[i][j];
            }
\end{lstlisting}
$ \sum_{j=0}^{n-1}{(8+3)} $ \\
Ahora consideramos el caso inicial del ciclo, y el final: $ 2 + \sum_{j=0}^{n}{(8+3)} + 3$ \\
Definimos:  $a \equiv 2 + \sum_{j=0}^{n-1}{(8+3)} + 3$ \\
$\textbf{Parte 2.1.2}$: 
\begin{lstlisting}
2 2 2   for(int i = 0; i < n; i++){
            a
        }
\end{lstlisting}
Comenzamos considerando el caso intermedio del ciclo.
$ \sum_{i=0}^{n}{(a+2)} $ \\
Ahora consideramos el caso inicial del ciclo, y el final:
$ 2 + \sum_{i=0}^{n}{(a+2)} + 2 $ \\ 
Definimos:  $b \equiv 2 + \sum_{i=0}^{n-1}{(a+2)} + 2 $ \\
$\textbf{Parte 2.1.3}$: Juntar ambos ciclos
$c \equiv 2 + \sum_{i=0}^{n-1}{((2 + \sum_{j=0}^{n-1}{(8+3)} + 3)+2)} + 2 $ \\ 
$\textbf{Parte 2.4}$: Resolver los ciclos, uniéndolos de alguna forma \\
$\equiv 2 + \sum_{i=0}^{n-1}{((2 + \sum_{j=0}^{n-1}{(\textbf{8+3})} + 3)+2)} + 2 $ \\ 
$\equiv 2 + \sum_{i=0}^{n-1}{((\textbf{2} + \sum_{j=0}^{n-1}{11} + \textbf{3})+2)} + 2 $ \\ 
$\equiv 2 + \sum_{i=0}^{n-1}{((\sum_{j=0}^{n-1}{\textbf{11}} + 5)+2)} + 2 $ \\ 
$\equiv 2 + \sum_{i=0}^{n-1}{((11 * \sum_{j=0}^{n-1}{1} + 5)+2)} + 2 $ \\
$\equiv \textbf{2} + \sum_{i=0}^{n-1}{((11 * n + 5)+2)} + \textbf{2} $ \\  
$\equiv 4 + \sum_{i=0}^{n-1}{(11 * n + 5+2)} $ \\  
$\equiv 4 + \sum_{i=0}^{n-1}{(11 * n + 7)} $ \\
$\equiv 4 + n * (11 * n + 7) $ \\    
$\equiv 4 + 11n^{2} + 7n $ \\   
$\textbf{Parte 2.1.5}$: Agregar el caso del return 
$\equiv 3 + 4 + 11n^{2} + 7n $ \\   
$\equiv 11n^{2} + 7n + 7 $ \\
$\textbf{Parte 2.1.6}$: Observar cual es la variable con mayor exponente.
En este caso, es $11n^{2}$ por lo tanto, $T{peor}(n) = \Theta(max\{11n^{2}, n, 7\})$. Luego,  
$T{peor}(n) = \Theta(n^{2})$ \\ 
Nota: En la materia no utilizan los números de operaciones, pero acá por ejemplo 2, 8, 3, 3, 2 (operaciones elementales) serían $\Theta(1)$, osea nuestro cálculo de la Parte 2.1.5 se vería así: $\Theta(n^{2}) + \Theta(n) + \Theta(1)$ \\ 
$\textbf{Parte 2.2: Mejor caso}$ \\ 
El mejor caso es igual al peor caso porque no tenemos restricciones del ciclo más que las matrices de entradas y tampoco tenemos ninguna condición de corte. Siempre se recorre la misma cantidad de veces.
\subsection*{Ejemplo 2 en cálculo de complejidad}
\begin{lstlisting}
   int russian(int a, int b){
        int res = 0;
        while(b > 0){
            if(b % 2 == 1){
                res = res + a;
            }
            a = a * 2;
            b = b/2;
        }
        return res;
   }
\end{lstlisting}
A simple vista podemos notar algo, estamos usando un while y la guarda depende de un b que desconocemos. Veamos como va variando la variable b; Se observa que se va dividiendo por dos, entonces es algo más rápido que hacer b-1. Seguramente, el peor caso sea logarítmico (como en búsqueda binaria que se va partiendo la lista en dos)
\newpage
\textbf{Parte 1}: Comenzamos colocando la cantidad de operaciones que se realizan, línea a línea. 
\begin{lstlisting}
    int russian(int a, int b){
        1    int res = 0;
        1    while(b > 0){
        2        if(b % 2 == 1){
        2           res = res + a;
                 }
        2        a = a * 2;
        2        b = b/2;
            }
        1   return res;
    }
 \end{lstlisting}
$\textbf{Parte 2}$: Elegimos si empezamos por el mejor, o el peor caso. El peor caso suele ser el más tedioso así que empecemos por ese. \\
$\textbf{Parte 2.1: Peor caso}$ \\
$\textbf{Parte 2.1.1}$: Observamos que nuestro peor caso es que b $>$ 0 y al dividir b por 2 sea impar. \\ Dentro del ciclo tenemos: (1 + (2+2) + 2 + 2) operaciones elementales. \\
Nótese que el 1 es de la condición de la guarda. \\ 
El ciclo es algo inverso, se comienza siendo algo muy grande y se va achicando, y el ciclo termina cuando b = 0. ¿Cuantas veces tengo que dividir b entre 2 para que podamos llegar a 0? El logaritmo base 2 de b, pero acá b lo llamamos n. \\ 
$\textbf{Parte 2.1.2}$: Utilizar logaritmo para hablar del ciclo \\
$\log(n)(1+(2+2) + 2 + 2)$ \\
Nótese que eliminamos la base 2 porque en complejidad, la base del logaritmo da igual.  \\
$\textbf{Parte 2.1.3}$: Considerar la asignación inicial y el return junto al ciclo \\ 
$1 + \log(n)(1+(2+2) + 2 + 2) + 1 \equiv 1 + \log(n)(8) + 1$ \\ 
\textbf{Parte 2.1.4}: Calcular la complejidad
$T_{peor}(n) = \Theta(1) + \Theta(log(n)) + \Theta(1) \equiv \Theta(max\{1, log(n), 1\}) \equiv \Theta(log(n)) $ \\
\textbf{Parte 2.2: Mejor caso} \\
Si b = 0 inicialmente, no entra nunca al ciclo por lo tanto no hay complejidad alguna porque todas son operaciones elementales. \\
$T_{mejor}(n) = \Theta(1) + \Theta(1) + \Theta(1) \equiv \Theta(max\{1, 1, 1\}) \equiv \Theta(1)$ \\


\end{document}
